{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opVkiIAWxIhh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.What is a parameter?**\n",
        "\n",
        "**Ans-** A parameter is a variable or value used to define or control a\n",
        "process, function, or system. Its meaning can vary depending on the context."
      ],
      "metadata": {
        "id": "wLGvG6yD0zFH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. What is correlation?\n",
        "What does negative correlation mean?**\n",
        "\n",
        "**Ans**- Correlation is a statistical measure that describes the strength and direction of a relationship between two variables. It shows how changes in one variable are associated with changes in another.\n",
        "  \n",
        "  A negative correlation occurs when two variables move in opposite directions. In other words, as one variable increases, the other decreases, and vice versa.\n",
        "  \n",
        "  Examples:\n",
        "\n",
        "1. Temperature and Hot Beverage Sales: As temperatures increase, the sales of hot beverages typically decrease.\n",
        "2. Exercise and Weight: More frequent exercise might lead to a decrease in weight (under certain conditions)."
      ],
      "metadata": {
        "id": "royuK-ol1h6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Define Machine Learning. What are the main components in Machine Learning?**\n",
        "\n",
        "**Ans-** Machine Learning (ML) is a subset of artificial intelligence (AI) that focuses on developing algorithms and models that allow computers to learn from data and make predictions or decisions without being explicitly programmed. In essence, it enables systems to improve their performance over time based on experience or new data.\n",
        "\n",
        " **Main Components in Machine Learning**\n",
        " 1. Data\n",
        "\n",
        "*  Definition: The foundation of ML, consisting of raw information used to train and test models.\n",
        "*  Types:\n",
        "Structured Data: Organized in rows and columns (e.g., databases, spreadsheets).\n",
        "*  Unstructured Data: Text, images, videos, etc.\n",
        "*  Quality and quantity of data directly impact the model's performance.\n",
        "\n",
        "2.  Features\n",
        "\n",
        "*   Definition: Individual measurable properties or characteristics of the data used for training the model.\n",
        "*  Feature Engineering:\n",
        "Selecting relevant features.\n",
        "Transforming data into formats suitable for algorithms (e.g., normalization, encoding).\n",
        "3. Model\n",
        "*  Definition: A mathematical representation or algorithm trained on data to make predictions or decisions.\n",
        "*  Examples: Linear regression, decision trees, neural networks.\n",
        "Models can be supervised, unsupervised, or reinforcement learning-based.\n",
        "Training.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YGOncJdv2xW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. How does loss value help in determining whether the model is good or not?**\n",
        "\n",
        "**Ans-**The loss value is a crucial metric in Machine Learning as it measures the difference between the predicted outputs of a model and the actual target values. It serves as an indicator of the model's performance during training and testing.\n",
        "  \n",
        "  **How Loss Helps in Evaluating the Model**\n",
        "\n",
        "   **1. Understanding Loss-**\n",
        "   *  Loss quantifies the error for a single prediction or batch of predictions.\n",
        "\n",
        "*  It is computed using a loss function, which varies depending on the task:\n",
        "\n",
        "*  Regression: Mean Squared Error (MSE), Mean Absolute Error (MAE).\n",
        "*  Classification: Cross-Entropy Loss, Hinge Loss.\n",
        "*  Lower loss indicates better predictions, while higher loss suggests the model needs improvement.\n",
        "\n",
        " **2.Role in Model Training**\n",
        "\n",
        "*  During training, the model minimizes the loss by adjusting its parameters (weights and biases) through an optimization algorithm like gradient descent.\n",
        "*  A low training loss usually implies the model is learning the patterns in the training data effectively.\n",
        "\n"
      ],
      "metadata": {
        "id": "KoOamZnQ4QyM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. What are continuous and categorical variables?**\n",
        "\n",
        "**Ans-** **1. Continuous Variables-**\n",
        "*  Definition: Continuous variables can take any value within a range and are typically measured on a scale. These variables are quantitative and represent data that can be divided into smaller units, often with meaningful decimal points.\n",
        "* Examples:\n",
        "\n",
        "\n",
        "*   Height (e.g., 175.3 cm)\n",
        "*   Temperature (e.g., 36.7Â°C)\n",
        "\n",
        "\n",
        "*   Time (e.g., 2.45 hours)\n",
        "*   Weight (e.g., 68.5 kg)\n",
        "\n",
        "2.  **Categorical Variables-**\n",
        "*  Definition: Categorical variables represent discrete groups or categories. These variables are qualitative and describe characteristics that cannot be meaningfully divided into smaller units.\n",
        "* Example -\n",
        "\n",
        "\n",
        "*   Gender (e.g., Male, Female, Non-binary)\n",
        "*   Blood Type (e.g., A, B, AB, O)\n",
        "*   Marital Status (e.g., Single, Married, Divorced)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TTgtLZUVQlsP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.How do we handle categorical variables in Machine Learning? What are the common techniques?**\n",
        "\n",
        "**Ans-**Handling categorical variables is an essential part of preparing data for machine learning models, as most models require numerical inputs. Below are common techniques for encoding and processing categorical variables.\n",
        "\n",
        "**1. Label Encoding-**\n",
        "*  Description: Converts each category into a unique integer label.\n",
        "*  Use Case: Useful when categorical variables have no inherent order (nominal data).\n",
        "\n",
        "**2. One-Hot Encoding-**\n",
        "*  Description:Converts categories into binary columns, where each column represents a category and contains 1 if the instance belongs to that category, 0 otherwise.\n",
        "*  Use Case: Works well for nominal data without order.\n",
        "\n",
        "**3. Ordinal Encoding-**\n",
        "*  Description: Assigns ordered integer labels to categories based on their rank or priority.\n",
        "*  Use Case: Suitable for ordinal data with meaningful order.\n",
        "\n",
        "**4. Frequency Encoding-**\n",
        "*  Description: Replaces categories with their frequency or proportion in the dataset.\n",
        "*  Use Case: Useful when the frequency of categories matters."
      ],
      "metadata": {
        "id": "Ev7nhXE_SRIW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. What do you mean by training and testing a dataset?**\n",
        "\n",
        "**Ans-**In machine learning, the terms \"training dataset\" and \"testing dataset\" refer to the data used in different phases of developing and evaluating a model. Here's what they mean:\n",
        "\n",
        "**1. Training Dataset-**\n",
        "*  Purpose:\n",
        "*  Used to teach the machine learning model by providing it with examples of input data and the corresponding correct outputs (labels in supervised learning).\n",
        "*  The model learns patterns, relationships, or rules from this data by adjusting its parameters (e.g., weights in neural networks).\n",
        "*  **Role in the Workflow:**\n",
        "*  The training dataset is the foundation for model building.\n",
        "*  The model's performance on this dataset reflects how well it has learned from the provided examples.\n",
        "*  **Characteristics:**\n",
        "*  Typically the largest portion of the data.\n",
        "*  Should be representative of the overall data distribution to ensure the model generalizes well.\n",
        "\n",
        "**2. Testing Dataset**\n",
        "*  Purpose:\n",
        "*  Used to evaluate the performance of the trained model on unseen data.\n",
        "*  Helps assess the model's ability to generalize to new inputs rather than just memorizing the training data."
      ],
      "metadata": {
        "id": "ZDVPRRAcWaSm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8.What is sklearn.preprocessing?**\n",
        "\n",
        "**Ans-** sklearn.preprocessing is a module in the scikit-learn library that provides tools for preprocessing and transforming data before using it in machine learning models. Preprocessing is an essential step in a machine learning pipeline to ensure that the data is in the right format and scale for efficient learning."
      ],
      "metadata": {
        "id": "zfqdZGOZa99C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9.What is a Test set?**\n",
        "\n",
        "**Ans-**A test set is a subset of the dataset used to evaluate the performance of a trained machine learning model. It consists of data that the model has never seen during training. The purpose of the test set is to measure how well the model generalizes to new, unseen data and to provide an unbiased estimate of its performance."
      ],
      "metadata": {
        "id": "fkW-pJ4MbWf_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10.How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem.**\n",
        "\n",
        "**Ans-**In Python, splitting data into training and testing sets is typically done using the train_test_split function from the sklearn.model_selection module. Here's how:\n",
        "\n",
        "**1.Using train_test_split**"
      ],
      "metadata": {
        "id": "RG5W98Bmbrpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Example dataset\n",
        "X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])\n",
        "y = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Output the results\n",
        "print(\"X_train:\", X_train)\n",
        "print(\"X_test:\", X_test)\n",
        "print(\"y_train:\", y_train)\n",
        "print(\"y_test:\", y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWJQJaCkfrHD",
        "outputId": "08705a32-ebd9-4efc-cb4a-007fcf0225d4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train: [[ 1]\n",
            " [ 8]\n",
            " [ 3]\n",
            " [10]\n",
            " [ 5]\n",
            " [ 4]\n",
            " [ 7]]\n",
            "X_test: [[9]\n",
            " [2]\n",
            " [6]]\n",
            "y_train: [0 1 0 1 1 0 1]\n",
            "y_test: [1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Parameters of train_test_split**\n",
        "*  test_size: Proportion of the dataset to be used as the test set (e.g., 0.3 for 30%).\n",
        "*  train_size: Proportion of the dataset to be used as the training set. Optional if test_size is specified.\n",
        "*  random_state: Ensures reproducibility by controlling the randomness of the split.\n",
        "*  stratify: Ensures the class distribution in the training and testing sets matches the original dataset. Useful for imbalanced datasets.\n",
        "\n",
        "**Approach to a Machine Learning Problem**\n",
        "\n",
        "**1. Understand the Problem**\n",
        "*  Define the Objective: What are you trying to predict or classify?\n",
        "*  Understand the Domain: Get familiar with the context of the problem.\n",
        "*  Identify the Type of Problem: Is it a regression, classification, clustering, or reinforcement learning problem.\n",
        "\n",
        "**2. Data Collection**\n",
        "\n",
        "* Gather relevant data from various sources.\n",
        "* Ensure data quality, completeness, and reliability"
      ],
      "metadata": {
        "id": "PRox7A6Af0YH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11.Why do we have to perform EDA before fitting a model to the data?**\n",
        "\n",
        "**Ans-**Performing Exploratory Data Analysis (EDA) before fitting a machine learning model is essential for ensuring that the model-building process is effective, robust, and efficient. EDA helps uncover the structure, patterns, and anomalies in the data.\n",
        "\n",
        "**Reasons to Perform EDA Before Fitting a Model**\n",
        "\n",
        "**1. Understand the Data**\n",
        "*  Objective: Get a clear understanding of the dataset, its features, and its overall structure.\n",
        "*  Why: It helps identify the nature of the problem (classification, regression, etc.) and the types of features (categorical, numerical, etc.).\n",
        "\n",
        "**2. Detect Missing Values**\n",
        "*  Objective: Identify incomplete or missing data.\n",
        "*  Why: Missing values can lead to errors or biased results if not handled properly."
      ],
      "metadata": {
        "id": "ExNv3GODiQyf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**12.What is correlation?**\n",
        "\n",
        "**Ans-**Correlation is a statistical measure that quantifies the strength and direction of the relationship between two variables. It indicates how one variable changes in relation to another. Correlation does not imply causation but is a useful tool for understanding patterns and associations in data."
      ],
      "metadata": {
        "id": "Qot6zpz6jW1l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**13.What does negative correlation mean?**\n",
        "\n",
        "**Ans-** A negative correlation between two variables means that as one variable increases, the other variable tends to decrease, and vice versa. In other words, the variables move in opposite directions. Negative correlation is represented by a correlation coefficient (\n",
        "ð\n",
        "r) that is less than\n",
        "0\n",
        "0, ranging from\n",
        "â\n",
        "1\n",
        "â1 to\n",
        "0\n",
        "0."
      ],
      "metadata": {
        "id": "J-aGTTEVoxcV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**14.How can you find correlation between variables in Python?**\n",
        "\n",
        "**Ans-**In Python, you can find the correlation between variables using various libraries, such as NumPy, Pandas, and Seaborn. The most common method is to use the corr() function in Pandas, which computes the pairwise correlation of columns in a DataFrame."
      ],
      "metadata": {
        "id": "TmJNeUarpWFT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**15.What is causation? Explain difference between correlation and causation with an example.**\n",
        "\n",
        "**Ans-**Causation refers to a direct cause-and-effect relationship between two variables, where a change in one variable directly causes a change in another. In causal relationships, the change in one variable is responsible for bringing about the change in the other variable.\n",
        "\n",
        "**Difference Between Correlation and Causation**\n",
        "\n",
        "**1. Correlation:**\n",
        "\n",
        "*  Definition: Correlation measures the strength and direction of a relationship between two variables, but it does not imply a direct cause-and-effect relationship.\n",
        "*  Key Point: Correlation simply shows that two variables are related, but it doesnât explain why or how the relationship occurs.\n",
        "*  Example: Ice cream sales and drowning incidents might be correlated during the summer. However, eating ice cream doesnât cause drowning.\n",
        "\n",
        "**2. Causation:**\n",
        "*  Definition: Causation occurs when one event (the cause) leads to the other event (the effect).\n",
        "*  Key Point: A causal relationship means that one variable directly influences the other.\n"
      ],
      "metadata": {
        "id": "LX8jzrPCpsZi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**16.What is an Optimizer? What are different types of optimizers? Explain each with an example.**\n",
        "\n",
        "**Ans-**An optimizer in machine learning and deep learning refers to an algorithm or method used to adjust the parameters (weights) of a model during training to minimize the loss function (or cost function). The goal is to find the optimal values for the model parameters that minimize the error between the predicted and actual outputs.\n",
        "\n",
        "Optimization plays a key role in training a model, and the process involves iterative adjustments to the model's parameters based on the gradient of the loss function with respect to those parameters.\n",
        "\n",
        "**Types of Optimizers**\n",
        "\n",
        "There are several types of optimizers, each with its own advantages and disadvantages. Below are the most common ones:\n",
        "\n",
        "**1. Gradient Descent (GD)**\n",
        "\n",
        "Gradient Descent is the most basic and widely used optimization algorithm. It works by updating the model's parameters in the direction of the negative gradient of the loss function.\n",
        "\n",
        "**2. Stochastic Gradient Descent (SGD)**\n",
        "\n",
        "Stochastic Gradient Descent (SGD) is a variant of gradient descent where the model parameters are updated using a single randomly chosen data point at each iteration, rather than the full batch of data.\n",
        "\n",
        "**Example:**\n",
        "If you're training a linear regression model with a dataset, instead of using the entire dataset to compute the gradient in each iteration, SGD would use one data point (or a small batch) per update.\n",
        "\n",
        "**3. Momentum**\n",
        "\n",
        "Momentum builds upon standard gradient descent by adding a \"velocity\" term that helps the optimization process maintain a constant direction. It can accelerate the convergence and reduce oscillations in the gradient updates.\n",
        "\n",
        "**Example:**\n",
        "In the context of SGD, momentum helps prevent the algorithm from oscillating in regions with shallow gradients and enables it to keep moving in the direction of steepest descent."
      ],
      "metadata": {
        "id": "g5KDLI8nsTo2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**17.What is sklearn.linear_model ?**\n",
        "\n",
        "**Ans-**sklearn.linear_model is a module in Scikit-learn (a popular machine learning library in Python) that contains a set of tools for linear models. Linear models are commonly used in supervised learning tasks, such as regression and classification, where the relationship between the input features and the output is assumed to be linear.\n",
        "\n",
        "The linear_model module provides various linear modeling techniques, including Linear Regression, Logistic Regression, and others like Ridge, Lasso, and ElasticNet for regularized linear models.\n",
        "\n"
      ],
      "metadata": {
        "id": "faYGajC9uPAl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**18.What does model.fit() do? What arguments must be given?**\n",
        "\n",
        "**Ans-**The model.fit() method in Scikit-learn is used to train a machine learning model. It adjusts the model's parameters (such as weights and biases) based on the provided training data to best capture the relationship between the input features (X) and the target labels (y). Once the model is trained, it can be used to make predictions on new, unseen data.\n",
        "\n",
        "**Arguments for model.fit()**\n",
        "\n",
        "**1. X_train (Required)**\n",
        "\n",
        "*  Type: array-like or pandas DataFrame or sparse matrix\n",
        "*  Description:\n",
        "* The feature matrix that represents the input data.\n",
        "* Each row corresponds to one data point (an observation), and each column corresponds to one feature (a variable) of the data.\n",
        "\n",
        "**2. y_train (Required)**\n",
        "\n",
        "* Type: array-like or pandas Series or sparse matrix\n",
        "* Description:\n",
        "* The target or label for each data point in the training dataset.\n",
        "* This is the value you want the model to predict or classify. It can be continuous (for regression tasks) or discrete (for classification tasks)."
      ],
      "metadata": {
        "id": "wrcY34h3ux7a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**19.What does model.predict() do? What arguments must be given?**\n",
        "\n",
        "**Ans-**The model.predict() method in Scikit-learn is used to make predictions using a trained machine learning model. Once the model has been trained with the model.fit() method, you can use model.predict() to apply the model to new, unseen data and generate predictions (outputs) based on the learned relationship between the input features and target labels.\n",
        "\n",
        "**Arguments for model.predict()**\n",
        "\n",
        "**1. X (Required):**\n",
        "\n",
        "*  Type: array-like, pandas DataFrame, or sparse matrix\n",
        "\n",
        "*  Description:\n",
        "\n",
        "*  The input feature matrix, representing the new data points for which you want the model to make predictions.\n",
        "*  This data must have the same number of features (columns) as the data used to train the model (i.e., X_train used in model.fit())."
      ],
      "metadata": {
        "id": "21Wxo5EA_SGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**20.What are continuous and categorical variables?**\n",
        "\n",
        "**Ans-**1. Continuous Variables: Continuous variables are numeric variables that can take an infinite number of values within a given range. These variables are typically measured on a scale and can include fractional or decimal values.\n",
        "\n",
        "2. Categorical Variables: Categorical variables represent data that can be grouped into categories or labels. They are often non-numeric but can be encoded as numbers for analysis."
      ],
      "metadata": {
        "id": "XMbo9pAvAUla"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**21.What is feature scaling? How does it help in Machine Learning?**\n",
        "\n",
        "**Ans-**Feature scaling is a data preprocessing technique used to standardize the range of independent variables (features) in a dataset. It ensures that all features contribute equally to the model's learning process by bringing them to a similar scale. This is crucial because many machine learning algorithms compute distances or gradients, which can be influenced by the range of feature values.\n",
        "\n",
        "1.Avoiding Bias Toward Larger-Scale Features:\n",
        "\n",
        "* Features with larger magnitudes (e.g., income in thousands vs. age in years) can dominate the training process, leading the model to give undue importance to those features.\n",
        "2. Improving Convergence:\n",
        "\n",
        "* Gradient-based algorithms like Gradient Descent converge faster when features are scaled because they operate more efficiently when all features are on similar scales.\n",
        "3. Better Performance in Distance-Based Algorithms:\n",
        "\n",
        "* Models like K-Nearest Neighbors (KNN), Support Vector Machines (SVM), and K-Means Clustering rely on distance metrics (e.g., Euclidean distance). If features are not scaled, larger-scale features can disproportionately influence the results.\n",
        "4. Enabling Proper Regularization:\n",
        "\n",
        "* Regularization techniques (e.g., L1, L2 regularization) penalize large coefficients. Without scaling, the penalty might not be applied proportionately to all features.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "j5fDgQbKAxvs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**22.How do we perform scaling in Python?**\n",
        "\n",
        "**Ans-** Python provides various libraries and methods to perform feature scaling, with Scikit-learn being the most commonly used library for preprocessing tasks.\n",
        "\n",
        "**Steps for Scaling**\n",
        "\n",
        "1.Import the Required Library: Use Scikit-learn's preprocessing module for scaling techniques.\n",
        "\n",
        "2.Choose the Appropriate Scaler: Select a scaler based on your data and the machine learning algorithm.\n",
        "\n",
        "3.Fit and Transform the Data:\n",
        "\n",
        "* Use the fit() method to compute the parameters required for scaling (e.g., mean, standard deviation).\n",
        "* Apply the transformation using the transform() method or combine both steps using fit_transform().\n",
        "4.Apply Scaling to Training and Test Data: Always fit the scaler on the training data, then transform both the training and test data to avoid data leakage.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "svKdkQwUBqQB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**23.What is sklearn.preprocessing?**\n",
        "\n",
        "**Ans-** sklearn.preprocessing is a module in the Scikit-learn library that provides various utilities and classes to preprocess data before applying machine learning algorithms. Preprocessing ensures that the data is properly formatted, scaled, and normalized to enhance the performance of machine learning models."
      ],
      "metadata": {
        "id": "ZHmitE3BCgqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**24.How do we split data for model fitting (training and testing) in Python?**\n",
        "\n",
        "**Ans-** To evaluate machine learning models effectively, it is crucial to split the data into training and testing sets. The training set is used to train the model, and the testing set is used to evaluate its performance on unseen data. This ensures that the model generalizes well to new data.\n",
        "\n",
        "**Steps to Split Data for Training and Testing**\n",
        "\n",
        "1.Import Required Libraries: Use train_test_split from Scikit-learn's model_selection module.\n",
        "\n",
        "2.Split Data: Specify the proportion of data to be used for training and testing.\n",
        "\n",
        "3.Set a Random State: Use a random seed to ensure reproducibility of result"
      ],
      "metadata": {
        "id": "GnjU9MebCtES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**25.Explain data encoding?**\n",
        "\n",
        "**Ans-** Data encoding in machine learning refers to the process of converting categorical data (non-numeric data) into a numerical format that can be understood by machine learning algorithms. Most algorithms require numerical inputs."
      ],
      "metadata": {
        "id": "pKdzEIa_DOtg"
      }
    }
  ]
}